{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a8c9a7-17ec-4d6f-8db8-bb9dc47993c1",
   "metadata": {},
   "source": [
    "Let's start with good old linear regression model. \n",
    "\n",
    "$ Y = X\\beta + \\epsilon $\n",
    "\n",
    "All of the members of the equation are matrices. Their dimensions are useful to know for the following calculations, so let's write them out:  \n",
    "\n",
    "$Y (nx1)$  \n",
    "$X (nxk)$  \n",
    "$\\beta (kx1)$  \n",
    "$\\epsilon (nx1)$  \n",
    "\n",
    "We have an interest in the realtionship between Xs and Y, thus we need to find $\\beta$\n",
    "One of the classical ways is to find it my minimizing the square error ($\\epsilon$) of the equation. The square error can be expressed like this  \n",
    "$\\epsilon^T\\epsilon = (Y-X\\beta)^T (Y-X\\beta)  \\equiv L$\n",
    "\n",
    "Notice that $\\epsilon^T\\epsilon$ is scalar value (1xn) (nx1) which can be minimized and it also implies that we are dealing with scalars on the right hand side. (I told you than knowing the dimensions would come in handy)\n",
    "\n",
    "$L = YY^T - Y^TX\\beta - X^T\\beta^TY + \\beta^TX^TX\\beta  $ (0)\n",
    "\n",
    "The second and third term are equvalent in value so we just can multiply any of them by 2.\n",
    "\n",
    "$$L = YY^T - 2X^T\\beta^TY + \\beta^TX^TX\\beta  $$\n",
    "\n",
    "Take the derivative and equate it to zero\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\beta}  = - 2X^TY - 2X^TX\\beta = 0  $ (1)  \n",
    "$ X^TX\\beta = X^TY  $  \n",
    "$ \\beta = (X^TX)^{(-1)}X^TY $  (2)\n",
    "\n",
    "A close form solution in thus found for $\\beta$ we can see it is nothing more than a combination of Xs and Ys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10cf8e3-06ea-4034-b80f-9d1ec53722a7",
   "metadata": {},
   "source": [
    "Let's do an example of a regression in the form of $y = 5.01 + 2.1*x_1 - 0.5*x_2 + \\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4338445c-084c-4852-b59e-2bb4f08bc025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "x1=np.array(range(0, 50)) + np.random.normal(size=(50))\n",
    "x2=np.random.normal(size=(50))\n",
    "x=np.stack((x1, x2)).T\n",
    "X = np.c_[np.ones((len(x), 1)), x]\n",
    "\n",
    "y = 5.01 + 2.1*x1 - 0.5*x2 + np.random.normal(loc=0.0, scale=0.2, size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f88d3d56-7a92-40f4-9b46-537d4cb241f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.02609659,  2.09905363, -0.55519427])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Equation (2)\n",
    "np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "#The solution is pretty close the values stated in the previous cell. The method works :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba5a83e-6d55-493e-afce-cfb0ac5096d6",
   "metadata": {},
   "source": [
    "Now let's see how the parameter estimation look like if esimate it through gradient descent.\n",
    "\n",
    "We have the equation (1) we can use  \n",
    "$\\frac{d L}{d \\beta} = \\nabla f = - 2X^TY - 2X^TX\\beta  $\n",
    "Let's just rewrite $X\\beta = \\hat{Y}$\n",
    "\n",
    "$\\nabla f   = - 2X^T(Y-\\hat{Y}) = 2X^T(\\hat{Y}-Y)   $ (3)\n",
    "\n",
    "And we thus have the gradient. But now the question remains on how to update the $\\beta$. Usully the equation is given without the derivation as  \n",
    "$\\beta_{t+1} = \\beta_{t} - lr*\\frac{\\partial L}{\\partial \\beta} $ (4) (t here denotes learing steps, meaning t+1 is the updated $\\beta$)\n",
    "\n",
    "However, in order to completely understand what is going on we can derive equation (4) \n",
    "\n",
    "We'll need Taylor's series expansion. Let's first remember how it look like in the general case and later use for gradient descent. Taylor series lets us to approximate a function locally by a polynomial order of our choosing. Let's say we have funciton $f(x)$ and expand it in point $x_{0}$\n",
    "\n",
    "$f(x) = \\frac{f(x{_0})}{0!}(x-x_{0})^0 + \\frac{f'(x{_0})}{1!}(x-x_{0})^1 + \\frac{f''(x{_0})}{2!}(x-x_{0})^2 + \\xi = \\frac{f^n(x{_0})}{n!}(x-x_{0})^n$\n",
    "\n",
    "Now let's do it for the function of $f(x+\\Delta x)$ in the point $x$\n",
    "\n",
    "$f(x+\\Delta x) \\approx f(x) + f'(x{_0})\\Delta x$\n",
    "\n",
    "now let's just replace our variables $\\beta_{t+1} \\equiv f( x+\\Delta x) $ and $\\beta_{t} \\equiv x $\n",
    "\n",
    "$f(\\beta_{t+1}) \\approx f(x) + f'(\\beta_{t})\\Delta x$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3356c92-9590-4b72-b30a-d259206891c5",
   "metadata": {},
   "source": [
    "So to summarize the process of gradient descent looks like this:\n",
    "\n",
    "0. Guess/initialize values of $\\beta$ to some (random) numbers\n",
    "1. Calculate the $\\hat{Y}$ with the current $\\beta$ (aka $\\beta_t$)\n",
    "2. Calculate the gradient with requires $\\hat{Y}$ we calculated in the first step which in turn requires the $\\beta$ \n",
    "3. Update the $\\beta_{t}$ using the equation (4), thus calculating  $\\beta_{t+1}$\n",
    "4. Repeat the steps 1-3 updating the $\\beta$ values until they stop improving (some tolerance level is reached)\n",
    "\n",
    "To make even clearer here is a schema on how the whole process works \n",
    "$$\\beta_t \\xrightarrow{\\text{calculate}} {\\hat{Y_t} \\xrightarrow{\\text{calculate}} \\nabla f \\xrightarrow{\\text{update}} \\beta_{t+1} }$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "541d39f7-4356-431f-aeb1-ad09d46d4f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, add_intercept = True, learningrate = 0.001, iterations = 100000):\n",
    "    y_real = np.reshape(Y, (len(Y), 1))   \n",
    "    cost_list = []\n",
    "    m=X.shape[0]\n",
    "\n",
    "    if add_intercept:\n",
    "        X = np.c_[np.ones((len(X), 1)), X]\n",
    "\n",
    "    beta = np.random.randn(X.shape[1], 1)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        #Equation (3)\n",
    "        gradient = 2/m * X.T.dot(X.dot(beta) - y_real)\n",
    "        #Equation (4)\n",
    "        beta = beta - learningrate * gradient\n",
    "        y_hat = X.dot(beta)\n",
    "        #Equation (0)\n",
    "        cost_value = 1/(2*m)*((y_hat - y)**2) \n",
    "        total = 0\n",
    "        for i in range(len(y)):\n",
    "            total += cost_value[i][0] \n",
    "        cost_list.append(total)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7516f871-8621-478a-a469-3e17112779d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.02609659],\n",
       "       [ 2.09905363],\n",
       "       [-0.55519427]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(x, y, add_intercept = True, learningrate = 0.001, iterations = 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac9f4fa-237f-44c2-96e7-0ff14d26c55a",
   "metadata": {},
   "source": [
    "The result is pretty close one again, so the algorithm works well at least for this case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19778b25-9e08-4251-9c1b-7ca0b1952ac7",
   "metadata": {},
   "source": [
    "${\\large Appendix}$\n",
    "\n",
    "More derivation without using matrix form, it may better explain the `gradient_descent` function above\n",
    "\n",
    "$$L(\\beta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "where:\n",
    "\n",
    "$m$ is the number of training examples\n",
    "$x^{(i)}$ and $y^{(i)}$ are the $i^{th}$ input-output pair\n",
    "$h(x) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n$ is the hypothesis function represented by the linear model\n",
    "$\\beta$ is the vector of parameters $(\\beta_0, \\beta_1, ..., \\beta_n)$\n",
    "Gradient of the cost function:\n",
    "\n",
    "$$\\nabla L(\\beta) = \\frac{1}{m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) x^{(i)}$$\n",
    "\n",
    "Parameter update rule:\n",
    "\n",
    "$$\\beta_{t+1} = \\beta_t - \\alpha \\frac{\\partial L(\\beta)}{\\partial \\beta_t}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d921f1d-12a8-422d-8b0d-ba71c38f9ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
